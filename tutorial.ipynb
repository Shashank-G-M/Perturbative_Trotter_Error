{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Shashank-G-M/Perturbative_Trotter_Error/blob/main/tutorial.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Installation of Packages\n",
        "Restart session and rerun the cell after installation to avoid numpy/scipy import errors"
      ],
      "metadata": {
        "id": "Vr-MimX0yfrq"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "_9855Ndjwc2y"
      },
      "outputs": [],
      "source": [
        "!pip install --prefer-binary pyscf==2.10.0\n",
        "!pip install openfermion==1.7\n",
        "!pip install openfermionpyscf==0.5\n",
        "!pip install opt_einsum==3.4.0\n",
        "\n",
        "# If you have access to GPU, some functions make use of python package CuPy. However, it is recommended to avoid this as it is experimental.\n",
        "# !pip install cupy"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Clone the Github repository\n",
        "\n",
        "If you want to save it in google drive for future use and make updates, first mount the drive and change the directory to a folder inside drive, then clone the repo. Otherwise, run the cell as is and it will clone the repo temporarily."
      ],
      "metadata": {
        "id": "ft-1GKd0FLxM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')\n",
        "# %cd /content/drive/MyDrive/your-folder          #Change your-folder with the name of the folder in drive where you want to clone the repo\n",
        "\n",
        "!git clone https://github.com/Shashank-G-M/Perturbative_Trotter_Error.git\n",
        "\n",
        "%cd Perturbative_Trotter_Error"
      ],
      "metadata": {
        "collapsed": true,
        "id": "fUfZC7luL_Rm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0d25a255"
      },
      "source": [
        "## Importing the Modules and Packages\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "71282f94",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "import scipy as sp\n",
        "from scipy.sparse.linalg import expm, eigs, eigsh\n",
        "from scipy.sparse import csc_matrix, csr_matrix, bsr_matrix\n",
        "from scipy.linalg import logm\n",
        "from scipy.optimize import minimize, Bounds\n",
        "from scipy.optimize import differential_evolution, NonlinearConstraint\n",
        "from scipy.stats import pearsonr\n",
        "\n",
        "from itertools import product\n",
        "from itertools import accumulate\n",
        "from itertools import permutations, combinations\n",
        "\n",
        "from functools import reduce\n",
        "from joblib import Parallel, delayed\n",
        "from copy import copy\n",
        "from opt_einsum import contract\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import time\n",
        "import math\n",
        "import random\n",
        "import pickle\n",
        "import h5py\n",
        "\n",
        "\n",
        "import openfermion as of\n",
        "from openfermion.hamiltonians import fermi_hubbard\n",
        "from openfermion.transforms import jordan_wigner, reverse_jordan_wigner\n",
        "from openfermion.linalg import qubit_operator_sparse\n",
        "from openfermion.linalg import get_number_preserving_sparse_operator\n",
        "from openfermion.utils import count_qubits, is_hermitian\n",
        "from openfermion import normal_ordered, chemist_ordered, commutator\n",
        "from openfermion import eigenspectrum\n",
        "from openfermion import get_sparse_operator\n",
        "from openfermion import FermionOperator, QubitOperator, MolecularData, hermitian_conjugated, get_molecular_data\n",
        "from openfermionpyscf import run_pyscf\n",
        "from openfermion.transforms import get_fermion_operator\n",
        "from openfermion import jw_hartree_fock_state\n",
        "from openfermion import MajoranaOperator\n",
        "from openfermion.hamiltonians import s_squared_operator, sz_operator, number_operator\n",
        "\n",
        "from pyscf import gto, scf, lo, fci\n",
        "from pyscf.cc.addons import spatial2spin\n",
        "\n",
        "import sys\n",
        "import os\n",
        "\n",
        "\n",
        "from pert_trotter.cost_utils import *\n",
        "from pert_trotter.tensor_utils import *\n",
        "from pert_trotter.ffrag_utils import *\n",
        "from pert_trotter.fock_utils import *\n",
        "from pert_trotter.ham_utils import *\n",
        "from pert_trotter.io_utils import *\n",
        "from pert_trotter.taper_utils import *\n",
        "from pert_trotter.trotter_utils import *\n",
        "from pert_trotter.fermi_frag import *\n",
        "from pert_trotter.error_pert import *\n",
        "from pert_trotter.qubit_utils import get_fc_group\n",
        "from pert_trotter.qubit_utils import get_qwc_group\n",
        "from pert_trotter.qubit_utils import get_greedy_grouping\n",
        "from pert_trotter.qubit_utils import Do_Qubit_Partitioning\n",
        "from pert_trotter import config"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k0gcB_iAVrG0"
      },
      "source": [
        "## Build molecular Hamiltonian\n",
        "\n",
        "All molecules except NH$_3$ are generated on the fly. NH$_3$ Hamiltonian, however, is loaded from the repository. To reproduce the results from the paper, use ```r=1``` for H$_2$, LiH, and BeH$_2$, and ```r=1.9``` for H$_2$O and NH$_3$.\n",
        "\n",
        "We demonstrate the usage for the case of LiH."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4489b978"
      },
      "outputs": [],
      "source": [
        "r = 1\n",
        "mol_name = 'lih'                                                                #Can be 'h2', 'beh2', 'h2o', and 'nh3'.\n",
        "\n",
        "if mol_name == 'nh3' and r == 1.9:\n",
        "  with open('data/mol_data/nh3_mol_data.pkl', 'rb') as in_file:\n",
        "    mol_data = pickle.load(in_file)\n",
        "  H = mol_data['FermionOperator']\n",
        "  num_elecs = mol_data['num_elecs']\n",
        "  n_qubits = count_qubits(H)\n",
        "  H_const, H_obt, H_tbt = get_chem_tensors(H)\n",
        "  H_obt_op, H_tbt_op = obt2op(H_obt), tbt2op(H_tbt)\n",
        "  Hchem = H_obt_op + H_tbt_op + H_const\n",
        "else:\n",
        "  H, num_elecs = obtain_OF_hamiltonian(mol_name, geometry=r)\n",
        "  n_qubits = count_qubits(H)\n",
        "  print (num_elecs, n_qubits)\n",
        "  H_const, H_obt, H_tbt = get_chem_tensors(H)\n",
        "  H_obt_op, H_tbt_op = obt2op(H_obt), tbt2op(H_tbt)\n",
        "  Hchem = H_obt_op + H_tbt_op + H_const\n",
        "\n",
        "config.mol_name = mol_name\n",
        "config.n_qubits = n_qubits\n",
        "config.num_elecs = num_elecs\n",
        "savepath = config.savepath\n",
        "\n",
        "JW_OF = jordan_wigner(Hchem)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Generate exact and approximate eigenstates in the full space"
      ],
      "metadata": {
        "id": "RNYMbVaFkjdM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### For non-tapered Hamiltonians\n",
        "\n",
        "Use this for smaller molecules i.e. all but NH$_3$. Note, diagonalization can be slow on colab, so switch to a personal computer for faster execution."
      ],
      "metadata": {
        "id": "OIDFjVl5knFg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Generating the exact eigenstates\n",
        "JW_OF_Array_sparse = qubit_operator_sparse(JW_OF)\n",
        "JW_OF_Array = JW_OF_Array_sparse.toarray()\n",
        "v, w = sp.linalg.eigh(JW_OF_Array)\n",
        "v0 = v[0]\n",
        "w0 = w[:,[0]]\n",
        "w0_sparse = sp.sparse.csc_matrix(w[:,[0]])\n",
        "\n",
        "# Generating the CISD approximated eigenstates\n",
        "proj_H, all_excitations = get_CI_proj_ham(JW_OF_Array, n_excitations = 2, ret_excitations = True, num_elecs = num_elecs, n_qubits = n_qubits)\n",
        "all_excitations_vec = np.zeros((2**n_qubits, len(all_excitations)))\n",
        "for i in range(len(all_excitations)):\n",
        "  all_excitations_vec[:,[i]] = occ_str_to_state(all_excitations[i])\n",
        "CISD_evals,CISD_evecs = np.linalg.eigh(proj_H)\n",
        "CISD_evecs_full_space = all_excitations_vec@CISD_evecs\n",
        "\n",
        "# Reordering the CISD states so that the first vector corresponds to S^2 = 0\n",
        "Ssq = s_squared_operator(n_qubits//2)\n",
        "Ssq_full_sparse = get_sparse_operator(Ssq, n_qubits)\n",
        "for i in range (len(CISD_evals)):\n",
        "  vec = sp.sparse.csc_matrix(CISD_evecs_full_space[:,[i]])\n",
        "  overlap = np.abs(vec.T*Ssq_full_sparse*vec)[0,0]\n",
        "  if np.round(overlap) == 0:\n",
        "    print ('Index of ground state of H with S^2 = 0: ', i)\n",
        "    break\n",
        "CISD_evecs_full_space[i], CISD_evecs_full_space[0] = CISD_evecs_full_space[0], CISD_evecs_full_space[i]\n",
        "CISD_evals[i], CISD_evals[0] = CISD_evals[0], CISD_evals[i]\n",
        "w0_sparse_CISD = sp.sparse.csc_matrix(CISD_evecs_full_space[:,[0]])\n",
        "\n",
        "print ('My CISD gs energy: ', CISD_evals[0])\n",
        "\n",
        "gs = CISD_evecs_full_space[:,[0]]\n",
        "\n",
        "\n",
        "\n",
        "#Saving the states in appropriate directory.\n",
        "os.makedirs(f'data/JW_OF/{mol_name}', exist_ok=True)          #Make directory if not present already\n",
        "\n",
        "with open('data/JW_OF/'+ mol_name + '/'+ mol_name + '_v', 'wb') as out_file:\n",
        "  pickle.dump(v, out_file)\n",
        "with open('data/JW_OF/'+ mol_name + '/'+ mol_name + '_w', 'wb') as out_file:\n",
        "  pickle.dump(w, out_file)\n",
        "with open('data/JW_OF/'+ mol_name + '/'+ mol_name + '_CISD_evals', 'wb') as out_file:\n",
        "  pickle.dump(CISD_evals, out_file)\n",
        "with open('data/JW_OF/'+ mol_name + '/'+ mol_name + '_CISD_evecs', 'wb') as out_file:\n",
        "  pickle.dump(CISD_evecs, out_file)\n",
        "with open('data/JW_OF/'+ mol_name + '/'+ mol_name + '_CISD_evecs_full_space', 'wb') as out_file:\n",
        "  pickle.dump(CISD_evecs_full_space, out_file)"
      ],
      "metadata": {
        "id": "NJPrD5hukfPu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### For tapered Hamiltonians\n",
        "\n",
        "Can be used for larger molecules. To reproduce the results for qubit based fragments of NH$_3$, use this method. The method can of course be used for smaller molecules as well."
      ],
      "metadata": {
        "id": "mriltbl1l5Pt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "load_ham = False                      # Set it to True when you have already saved the tapered Hamiltonian\n",
        "save_ham = True                      # Set it to True when you want to save the tapered Hamiltonian\n",
        "GPU = False                           # Experimental.\n",
        "\n",
        "\n",
        "#Make directory if not present already\n",
        "os.makedirs(savepath + 'tapered_JW_OF/' + mol_name, exist_ok=True)\n",
        "\n",
        "if load_ham == True:\n",
        "  savepath = 'data/'\n",
        "  with open(savepath + 'tapered_JW_OF/' + mol_name + '/' + mol_name + '_ham_ndarray', 'rb') as in_file:\n",
        "    tapered_JW_OF_array = pickle.load(in_file)\n",
        "  # tapered_JW_OF_sarray = sp.sparse.csc_matrix(tapered_JW_OF_array)\n",
        "  ref_det = '1'*num_elecs + '0'*(n_qubits-num_elecs)\n",
        "else:\n",
        "  ref_det = '1'*num_elecs + '0'*(n_qubits-num_elecs)\n",
        "  tapered_JW_OF = taper_qubits(JW_OF, n_qubits, int(num_elecs/2), int(num_elecs/2))\n",
        "  tapered_JW_OF_sarray = get_sparse_operator(tapered_JW_OF, n_qubits-2)\n",
        "  tapered_JW_OF_array = np.real(tapered_JW_OF_sarray).toarray()\n",
        "  if save_ham == True:\n",
        "    savepath = 'data/'\n",
        "    with open(savepath + 'tapered_JW_OF/' + mol_name + '/' + mol_name + '_ham_ndarray', 'wb') as out_file:\n",
        "      pickle.dump(tapered_JW_OF_array, out_file)\n",
        "\n",
        "\n",
        "if GPU == True:\n",
        "  tapered_JW_OF_cparray = cp.asarray(tapered_JW_OF_array)\n",
        "  tap_v_cp, tap_w_cp = cp.linalg.eigh(tapered_JW_OF_cparray)\n",
        "  tap_v = cp.asnumpy(tap_v_cp)\n",
        "  tap_w = cp.asnumpy(tap_w_cp)\n",
        "else:\n",
        "  tap_v, tap_w = np.linalg.eigh(tapered_JW_OF_array)\n",
        "\n",
        "tap_w0_sparse = sp.sparse.csc_matrix(tap_w[:,[0]])\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "tapered_JW_OF_sarray = sp.sparse.csc_matrix(tapered_JW_OF_array)\n",
        "tapered_CI_states = get_tapered_CI_states(ref_det, [0,1,2], preserve_Sz=False)\n",
        "CIproj_tap_JW_OF_sarray = tapered_CI_states.T*tapered_JW_OF_sarray*tapered_CI_states\n",
        "\n",
        "CIproj_tap_v0 = eigsh(CIproj_tap_JW_OF_sarray, k = 1, which='SA', return_eigenvectors=False)\n",
        "tap_v0 = eigsh(tapered_JW_OF_sarray, k = 1, which='SA', return_eigenvectors=False)\n",
        "\n",
        "\n",
        "tap_CISD_evals, tap_CISD_evecs = np.linalg.eigh(CIproj_tap_JW_OF_sarray.toarray())\n",
        "tap_CISD_evecs_full_space = tapered_CI_states*tap_CISD_evecs\n",
        "\n",
        "\n",
        "with open(savepath + 'tapered_JW_OF/' + mol_name + '/' + mol_name + '_tap_v', 'wb') as in_file:\n",
        "  pickle.dump(tap_v, in_file)\n",
        "\n",
        "dump_ndarray_h5(savepath + 'tapered_JW_OF/' + mol_name + '/' + mol_name + '_tap_w.h5', tap_w)\n",
        "\n",
        "\n",
        "with open(savepath + 'tapered_JW_OF/' + mol_name + '/' + mol_name + '_tap_CISD_evals', 'wb') as in_file:\n",
        "  pickle.dump(tap_CISD_evals, in_file)\n",
        "\n",
        "with open(savepath + 'tapered_JW_OF/' + mol_name + '/' + mol_name + '_tap_CISD_evecs_full_space', 'wb') as in_file:\n",
        "  pickle.dump(tap_CISD_evecs_full_space, in_file)"
      ],
      "metadata": {
        "id": "w75_cxoml7IH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lvXddtgk74Qh"
      },
      "source": [
        "## Building projector: $N = \\eta,\\ S_z=0,\\ $and$\\ S^2 = 0$\n",
        "\n",
        "Unliike qubit fragments, fermionic fragments have the same symmetry as the Hamiltonian. This enables us to perform calculations in the symmetry subspace of the ground state of the Hamiltonian. In this section, we build the projector onto this subspace. We also project CISD approximated eigenstates onto this subspace."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GwPgkSbu0AVD"
      },
      "outputs": [],
      "source": [
        "Ssq = s_squared_operator(n_qubits//2)\n",
        "Ssq_sparse = get_number_preserving_sparse_operator(Ssq, n_qubits, num_elecs, spin_preserving=True)\n",
        "\n",
        "Ssq_array = Ssq_sparse.toarray()\n",
        "\n",
        "Ssq_v, Ssq_w = np.linalg.eigh(Ssq_array)\n",
        "Ssq_w_sparse = sp.sparse.csc_matrix(Ssq_w)\n",
        "# Ssq_w0_sparse = sp.sparse.csc_matrix(Ssq_w[:,[0]])\n",
        "\n",
        "counter = 0\n",
        "for i in range(len(Ssq_v)):\n",
        "    if Ssq_v[i]<= 0.01:\n",
        "        counter += 1\n",
        "non_cisd_dim = counter\n",
        "\n",
        "Ssq_evals, NSz2SSq_Proj = Ssq_v[:counter], Ssq_w[:, :counter].T\n",
        "NSz2SSq_Proj_sparse = sp.sparse.csc_matrix(NSz2SSq_Proj)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#Projeector within CISD space\n",
        "Ssq_CISD_sparse = get_number_preserving_sparse_operator(Ssq, n_qubits, num_elecs, spin_preserving=True, excitation_level=2)\n",
        "Ssq_CISD_array = Ssq_CISD_sparse.toarray()\n",
        "\n",
        "Ssq_CISD_v, Ssq_CISD_w = np.linalg.eigh(Ssq_CISD_array)\n",
        "Ssq_CISD_w_sparse = sp.sparse.csc_matrix(Ssq_CISD_w)\n",
        "Ssq_CISD_w0_sparse = sp.sparse.csc_matrix(Ssq_CISD_w[:,[0]])\n",
        "\n",
        "counter = 0\n",
        "for i in range(len(Ssq_CISD_v)):\n",
        "    if Ssq_CISD_v[i]<= 0.01:\n",
        "        counter += 1\n",
        "cisd_dim = counter\n",
        "\n",
        "Ssq_CISD_evals, NSz2SSq_CISD_Proj = Ssq_CISD_v[:counter], Ssq_CISD_w[:, :counter].T\n",
        "NSz2SSq_CISD_Proj_sparse = sp.sparse.csc_matrix(NSz2SSq_CISD_Proj)\n",
        "\n",
        "\n",
        "\n",
        "def get_projected_sparse_op(H_OF = FermionOperator | QubitOperator, n_qubits = n_qubits, num_elecs = num_elecs, spin_preserving = True, excitation_level = None, reference_determinant=None): #H_OF is a FermioOperator in full space\n",
        "  if type(H_OF) == QubitOperator:\n",
        "    H_OF = normal_ordered(reverse_jordan_wigner(H_OF))\n",
        "  first_projected_op = get_number_preserving_sparse_operator(H_OF, n_qubits, num_elecs, spin_preserving=spin_preserving, excitation_level=excitation_level, reference_determinant=reference_determinant)\n",
        "  if excitation_level == None:\n",
        "    return NSz2SSq_Proj_sparse*first_projected_op*NSz2SSq_Proj_sparse.T\n",
        "  elif excitation_level == 2:\n",
        "    return NSz2SSq_CISD_Proj_sparse*first_projected_op*NSz2SSq_CISD_Proj_sparse.T\n",
        "  else:\n",
        "    print ('Invalid excitation level. Should be 2 or None.')\n",
        "    return None\n",
        "\n",
        "def get_projected_sparse_vec(vec, CISD = False): # vec must be a M x 1 sparse operator where M is the dimension of N = \\eta, Sz = 0 subspace.\n",
        "  if CISD == False:\n",
        "    return NSz2SSq_Proj_sparse*vec\n",
        "  else:\n",
        "    return NSz2SSq_CISD_Proj_sparse*vec\n",
        "\n",
        "\n",
        "\n",
        "H_Proj_sparse = get_projected_sparse_op(Hchem, n_qubits, num_elecs, spin_preserving=True)\n",
        "H_Proj_Array = H_Proj_sparse.toarray()\n",
        "#Hamiltonian eigenstates projected onto number of electrons = num_elecs, Sz = 0, and S^2 = 0 subspace\n",
        "sym_v, sym_w = np.linalg.eigh(H_Proj_Array)\n",
        "sym_w0_sparse = sp.sparse.csc_matrix(sym_w[:,[0]])\n",
        "\n",
        "\n",
        "H_NSz_CISD = get_number_preserving_sparse_operator(Hchem, n_qubits, num_elecs, spin_preserving=True, excitation_level=2)\n",
        "H_NSz_CISD_v, H_NSz_CISD_w = np.linalg.eigh(H_NSz_CISD.toarray())\n",
        "correct_zeros = np.zeros((len(Ssq_v) - len(H_NSz_CISD_v), len(H_NSz_CISD_v)))\n",
        "H_NSz_CISD_full_space_w = np.vstack((H_NSz_CISD_w, correct_zeros))\n",
        "#CISD approx to Hamiltonian eigenstates projected onto number of electrons = num_elecs, Sz = 0, and S^2 = 0 subspace\n",
        "H_NSzSsq_CISD_full_space_w = NSz2SSq_Proj@H_NSz_CISD_full_space_w\n",
        "H_NSzSsq_CISD_full_space_w0_sparse = sp.sparse.csc_matrix(H_NSzSsq_CISD_full_space_w[:,[0]])\n",
        "\n",
        "#CISD approximated eigenstates which do not have S^2 = 0 will be columns of H_NSzSsq_CISD_full_space_w with ~ 0 everywhere."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xFw8vk0gfw7P"
      },
      "source": [
        "## Generating Hamiltonian Partitioning\n",
        "\n",
        "We generate 4 kinds of qubit Hamiltonian fragments (QWCSI, QWCLF, FCSI, FCLF) and 5 kinds of fermionic Hamiltonian fragments (LRLCU, GFROLCU, LR, GFRO, SDGFRO). Refer to Appendix A of the paper for more detials."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PDLiESzhoGR2"
      },
      "source": [
        "### Non tapered\n",
        "For large systems (Ex: NH$_3$), uncomment the last line to store the data as a hdf5 file instad of a pickle dump."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "nvQ05i1Kf0WQ"
      },
      "outputs": [],
      "source": [
        "save, load, spacial = True, True, True\n",
        "\n",
        "method = 'lr'\n",
        "\n",
        "#Saving the data in appropriate directory.\n",
        "sarray_savepath = savepath + method + '/sparse arrays/'\n",
        "os.makedirs(sarray_savepath + 'Full space', exist_ok=True)          #Make directory if not already present\n",
        "\n",
        "if method in ['lr', 'gfro', 'lrlcu', 'gfrolcu', 'sdgfro']:\n",
        "  fragments_list_sarray = Do_Fermi_Partitioning (Hchem, type = method, tol=1e-6, spacial = spacial, save = save, load = load, projector_func = get_projected_sparse_op)\n",
        "else:\n",
        "  fragments_list = Do_Qubit_Partitioning (JW_OF, type = method)\n",
        "  #IMPORTANT!!!!! For Sorted insertion based methods (qwcsi and fcsi), identity operator must be added manually to the first fragment.\n",
        "  if method in ['qwcsi', 'fcsi']:\n",
        "    fragments_list[0] += JW_OF.constant*QubitOperator.identity()\n",
        "  with open(savepath + method + '/' + mol_name +'_'  + method + '_frag_ops.pkl', 'wb') as out_file:\n",
        "    pickle.dump(fragments_list, out_file)\n",
        "  fragments_list_sarray = [get_sparse_operator(frag, n_qubits) for frag in fragments_list]\n",
        "\n",
        "\n",
        "with open(sarray_savepath + 'Full space/' + mol_name + '_sarray.pkl', 'wb') as in_file:\n",
        "  pickle.dump(fragments_list_sarray, in_file)\n",
        "# dump_list_of_sparse2_h5(sarray_savepath + 'Full space/' + mol_name + '_sarray.h5', fragments_list_sarray)             #Useful when the size of the file is too large."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "25fIR6NgoIhp"
      },
      "source": [
        "### Tapered"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QFGdt5TXwqAx"
      },
      "outputs": [],
      "source": [
        "tapered_JW_OF = taper_qubits(JW_OF, n_qubits, int(num_elecs/2), int(num_elecs/2))\n",
        "method = 'qwclf'\n",
        "\n",
        "\n",
        "#Saving the data in appropriate directory.\n",
        "sarray_savepath = savepath + 'tapered_' + method + '/sparse arrays/'\n",
        "os.makedirs(sarray_savepath + 'Full space', exist_ok=True)          #Make directory if not already present\n",
        "\n",
        "\n",
        "fragments_list = Do_Qubit_Partitioning(tapered_JW_OF, type = method)\n",
        "#IMPORTANT!!!!! For Sorted insertion based methods (qwcsi and fcsi), manually add the appropriate identity operator to the first fragment, as they lack the identity.\n",
        "if method in ['qwcsi', 'fcsi']:\n",
        "  fragments_list[0] += tapered_JW_OF.constant*QubitOperator.identity()\n",
        "with open(savepath + 'tapered_' + method + '/' + mol_name +'_'  + method + '_frag_ops.pkl', 'wb') as out_file:\n",
        "  pickle.dump(fragments_list, out_file)\n",
        "\n",
        "\n",
        "fragments_list_sarray = [get_sparse_operator(frag, n_qubits-2) for frag in fragments_list]\n",
        "sarray_savepath = savepath + 'tapered_' + method + '/sparse arrays/'\n",
        "dump_list_of_sparse2_h5(sarray_savepath + 'Full space/' + mol_name + '_sarray.h5', fragments_list_sarray)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PeMAOPwpmPnu"
      },
      "source": [
        "## Calculate exact and approximate perturbative Trotter error $(ε_2,\n",
        " ε_{\\text{app}})$\n",
        "\n",
        " Assumes you have generated and stored relevant fragments and eigenstates using the above code blocks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cNjCIfKFvY6e"
      },
      "source": [
        "### For fermionic fragments"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zl5luwHsvaLZ"
      },
      "outputs": [],
      "source": [
        "method = 'lr'\n",
        "sarray_savepath = savepath + method + '/sparse arrays/'\n",
        "\n",
        "\n",
        "try:\n",
        "  fragments_list_sarray = list(load_h5_list_of_sparse(sarray_savepath + 'Full space/' + mol_name + '_sarray.h5'))\n",
        "except FileNotFoundError:\n",
        "  with open(sarray_savepath + 'Full space/' + mol_name + '_sarray.pkl', 'rb') as in_file:\n",
        "    fragments_list_sarray = list(pickle.load(in_file))\n",
        "\n",
        "\n",
        "v, w, w0_sparse = sym_v, sym_w, sym_w0_sparse\n",
        "CISD_v, CISD_w, CISD_w0_sparse = H_NSz_CISD_v, H_NSzSsq_CISD_full_space_w, H_NSzSsq_CISD_full_space_w0_sparse\n",
        "\n",
        "\n",
        "\n",
        "#Second Order Trotter\n",
        "print ('Full space (\\u03F5_2):')\n",
        "v2_contri = efficient_v2_contri_2_order_Trotter(fragments_list_sarray, w0_sparse)\n",
        "print ('2nd order Trotter v2 contribution: ', v2_contri)\n",
        "\n",
        "\n",
        "print ('\\n\\nCISD space (\\u03F5_app):')\n",
        "v2_contri = efficient_v2_contri_2_order_Trotter(fragments_list_sarray, CISD_w0_sparse)\n",
        "print ('2nd order Trotter v2 contribution: ', v2_contri)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AweGzufBvjJ0"
      },
      "source": [
        "### For qubit fragments"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "laiZav86e6e_"
      },
      "outputs": [],
      "source": [
        "method = 'qwclf'\n",
        "mol_name = 'lih'\n",
        "istapered = True                       # Setting this to True assumes you have generated and saved the tapered data for the current molecule\n",
        "\n",
        "\n",
        "if istapered == True:\n",
        "  sarray_savepath = savepath + 'tapered_' + method + '/sparse arrays/'\n",
        "  fragments_list_sarray = load_h5_list_of_sparse(sarray_savepath + 'Full space/' + mol_name + '_sarray.h5')\n",
        "\n",
        "  with open(savepath + 'tapered_JW_OF/' + mol_name + '/' + mol_name + '_tap_v', 'rb') as in_file:\n",
        "    v = pickle.load(in_file)\n",
        "  w = load_h5_ndarray(savepath + 'tapered_JW_OF/' + mol_name + '/' + mol_name + '_tap_w.h5')\n",
        "  w0_sparse = sp.sparse.csc_matrix(w[:,[0]])\n",
        "\n",
        "  with open(savepath + 'tapered_JW_OF/' + mol_name + '/' + mol_name + '_tap_CISD_evals', 'rb') as in_file:\n",
        "    CISD_v = pickle.load(in_file)\n",
        "  with open(savepath + 'tapered_JW_OF/' + mol_name + '/' + mol_name + '_tap_CISD_evecs_full_space', 'rb') as in_file:\n",
        "    CISD_w = pickle.load(in_file)\n",
        "  CISD_w0_sparse = sp.sparse.csc_matrix(CISD_w[:,[0]])\n",
        "else:\n",
        "  sarray_savepath = savepath + method + '/sparse arrays/'\n",
        "  with open(sarray_savepath + 'Full space/' + mol_name + '_sarray.pkl', 'rb') as in_file:\n",
        "    fragments_list_sarray = pickle.load(in_file)\n",
        "\n",
        "  with open(savepath + 'JW_OF/' + mol_name + '/' + mol_name + '_v', 'rb') as in_file:\n",
        "    v = pickle.load(in_file)\n",
        "\n",
        "  try:\n",
        "    w = load_h5_ndarray(savepath + 'JW_OF/' + mol_name + '/' + mol_name + '_w.h5')\n",
        "  except FileNotFoundError:\n",
        "    with open(savepath + 'JW_OF/' + mol_name + '/' + mol_name + '_w', 'rb') as in_file:\n",
        "      w = pickle.load(in_file)\n",
        "  w0_sparse = sp.sparse.csc_matrix(w[:,[0]])\n",
        "\n",
        "  with open(savepath + 'JW_OF/' + mol_name + '/' + mol_name + '_CISD_evals', 'rb') as in_file:\n",
        "    CISD_v = pickle.load(in_file)\n",
        "  with open(savepath + 'JW_OF/' + mol_name + '/' + mol_name + '_CISD_evecs_full_space', 'rb') as in_file:\n",
        "    CISD_w = pickle.load(in_file)\n",
        "  CISD_w0_sparse = sp.sparse.csc_matrix(CISD_w[:,[0]])\n",
        "\n",
        "\n",
        "#Second order Trotter\n",
        "print ('Full space (\\u03F5_2):')\n",
        "v2_contri = efficient_v2_contri_2_order_Trotter(fragments_list_sarray, w0_sparse)\n",
        "print ('2nd order Trotter v2 contribution: ', v2_contri)\n",
        "\n",
        "print ('\\n\\nCISD space (\\u03F5_app):')\n",
        "v2_contri = efficient_v2_contri_2_order_Trotter(fragments_list_sarray, CISD_w0_sparse)\n",
        "print ('2nd order Trotter v2 contribution: ', v2_contri)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O9WDZJIfkfqZ"
      },
      "source": [
        "## Evaluating $\\alpha$\n",
        "\n",
        "Refer to Eq. (4) of the paper for the definition of $\\alpha$"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# # import cupy as cp\n",
        "# # import cupyx.scipy.sparse as cusparse\n",
        "# # from cupyx.scipy.sparse.linalg import eigsh as gpu_eigsh\n",
        "\n",
        "method = 'lr'\n",
        "mol_name = 'lih'\n",
        "istapered = False\n",
        "\n",
        "sarray_savepath = savepath + method + '/sparse arrays/'\n",
        "\n",
        "if istapered == True:\n",
        "  try:\n",
        "    fragments_list_sarray = load_h5_list_of_sparse(savepath + 'tapered_' + method + '/sparse arrays/Full space/' + mol_name + '_sarray.h5')\n",
        "  except FileNotFoundError:\n",
        "    with open(savepath + 'tapered_' + method + '/sparse arrays/Full space/' + mol_name + '_sarray.pkl', 'rb') as in_file:\n",
        "      fragments_list_sarray = pickle\n",
        "else:\n",
        "  try:\n",
        "    fragments_list_sarray = load_h5_list_of_sparse(sarray_savepath + 'Full space/' + mol_name + '_sarray.h5')\n",
        "  except FileNotFoundError:\n",
        "    with open(sarray_savepath + 'Full space/' + mol_name + '_sarray.pkl', 'rb') as in_file:\n",
        "      fragments_list_sarray = pickle.load(in_file)\n",
        "\n",
        "# fragments_list_sarray_gpu = [cusparse.csc_matrix(frag, dtype=cp.complex128) for frag in fragments_list_sarray]\n",
        "\n",
        "alpha = Trotter_2nd_order_alpha_error(fragments_list_sarray, gpu=False)\n",
        "print (\"\\u03B1 =\", alpha)"
      ],
      "metadata": {
        "id": "bkgBNLLA_QH7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6-OvwBjNo10u"
      },
      "source": [
        "## Working with Trotter Unitary\n",
        "\n",
        "Generate U_trotter and get the effective Hamiltonian from it by applying matrix logarithm. This cell also calculates all other relevant quantities concerning U_trotter and $\\hat{H}_{\\text{eff}}$ such as, $\\alpha_e$ (see Eq. (10) in paper),$\\ \\varepsilon,\\ E_0,\\ E_0^{T},\\ \\Delta E_T,\\ $and time step $t$ or ($dt$). \\\n",
        "_Caution: For large molecules and qubit based methods, this step could take a while._"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "method = 'lr'\n",
        "mol_name = 'lih'\n",
        "istapered = False\n",
        "\n",
        "if istapered == True:\n",
        "  try:\n",
        "    fragments_list_sarray = load_h5_list_of_sparse(savepath + 'tapered_' + method + '/sparse arrays/Full space/' + mol_name + '_sarray.h5')\n",
        "  except FileNotFoundError:\n",
        "    with open(savepath + 'tapered_' + method + '/sparse arrays/Full space/' + mol_name + '_sarray.pkl', 'rb') as in_file:\n",
        "      fragments_list_sarray = pickle\n",
        "else:\n",
        "  try:\n",
        "    fragments_list_sarray = load_h5_list_of_sparse(savepath + method + '/sparse arrays/' + 'Full space/' + mol_name + '_sarray.h5')\n",
        "  except FileNotFoundError:\n",
        "    with open(savepath + method + '/sparse arrays/' + 'Full space/' + mol_name + '_sarray.pkl', 'rb') as in_file:\n",
        "      fragments_list_sarray = pickle.load(in_file)\n",
        "\n",
        "\n",
        "frags_len = len(fragments_list_sarray)\n",
        "\n",
        "Happ = sp.sparse.csc_matrix(sum(fragments_list_sarray))\n",
        "v0 = sp.sparse.linalg.eigsh(Happ, k=1, which='SA', tol = 1e-8, return_eigenvectors=False)[0]\n",
        "spec_norm = np.abs(sp.sparse.linalg.eigsh(Happ, k=1, which='LM', tol = 1e-8, return_eigenvectors=False)[0])\n",
        "dt = 1/spec_norm\n",
        "\n",
        "\n",
        "\n",
        "U_trotter = sp.sparse.eye(Happ.shape[0], format = 'csc')\n",
        "counter = 1\n",
        "for frag in fragments_list_sarray:\n",
        "  print ('scipy exponentiation initiated (via sp.sparse.linalg.expm)')\n",
        "  U_trotter_frag = sp.sparse.linalg.expm(-1.j*frag*dt/2)                        #dt/2 because we are eventually going to construct second order Trotter unitary\n",
        "  print ('scipy exponentiation complete')\n",
        "\n",
        "  U_trotter = U_trotter*U_trotter_frag\n",
        "\n",
        "  print (f'Loop : {counter}/{frags_len} \\n')\n",
        "  counter += 1\n",
        "U_trotter_array = U_trotter.toarray()\n",
        "\n",
        "\n",
        "\n",
        "U_trotter_array_2nd_order = U_trotter_array@U_trotter_array.T\n",
        "H_eff_array_2nd_order = 1.j*sp.linalg.logm(U_trotter_array_2nd_order)/(dt)\n",
        "H_eff_sarray_2nd_order = sp.sparse.csc_matrix(H_eff_array_2nd_order)\n",
        "\n",
        "\n",
        "v0_eff = sp.sparse.linalg.eigsh(H_eff_sarray_2nd_order, k=1, which='SA', tol = 1e-8, return_eigenvectors=False)[0]\n",
        "\n",
        "DE_T = np.abs(v0_eff - v0)\n",
        "epsilon = DE_T/dt**2\n",
        "\n",
        "print ('Epsilon = ', epsilon)\n",
        "\n",
        "U_trotter_sarray_2nd_order = sp.sparse.csc_matrix(U_trotter_array_2nd_order)\n",
        "\n",
        "U_exact_sarray = sp.sparse.linalg.expm(-1.j*Happ*dt)\n",
        "alpha_e_op = U_exact_sarray - U_trotter_sarray_2nd_order\n",
        "alpha_e = np.abs(sp.sparse.linalg.eigs(alpha_e_op, k=1, which='LM', return_eigenvectors = False)[0])\n",
        "alpha_e = alpha_e/dt**3\n",
        "\n",
        "print ('alpha_e = ', alpha_e)\n",
        "\n",
        "print ('dt = ', dt)\n",
        "\n",
        "H_eff_info = {'mol_name': mol_name, 'fragmentation': method, 'E0': v0, 'E0_eff': v0_eff, 'DeltaE_T': DE_T, 'epsilon': epsilon, 'alpha_e': alpha_e, 'time_step': dt}\n",
        "\n",
        "print (H_eff_info)\n",
        "\n",
        "if istapered:\n",
        "  with open(savepath + 'tapered_' + method + '/sparse arrays/' + 'Full space/' + mol_name + '_H_eff_info.pkl', 'wb') as out_file:\n",
        "    pickle.dump(H_eff_info, out_file)\n",
        "else:\n",
        "  with open(savepath + method + '/sparse arrays/' + 'Full space/' + mol_name + '_H_eff_info.pkl', 'wb') as out_file:\n",
        "    pickle.dump(H_eff_info, out_file)"
      ],
      "metadata": {
        "id": "U-AhL7-AjwXe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DwaNvGpPnWII"
      },
      "source": [
        "## Correlation Plots\n",
        "\n",
        "Producing figure 1 of the paper using tables provided in the paper. All the values in the tables can be generated using the cells above."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib\n",
        "\n",
        "a = 0\n",
        "b = 5\n",
        "\n",
        "c = 0\n",
        "d = 9\n",
        "\n",
        "matplotlib.rcParams.update({'font.size': 13})\n",
        "\n",
        "eps_v2_exact = np.array([\n",
        "    [0.0032412139, 0.0032412139, 0.0032412139, 0.0032412139, 0.0032412139, 0.0032412139, 0.0027744548, 0.002780725, 0.00300493],\n",
        "    [0.0032480968, 0.002163644, 0.0030127983, 0.00244501, 0.003300397, 0.003386966, 0.04718898, 0.049932963, 0.01818811],\n",
        "    [0.013731045, 0.011020856, 0.02272475, 0.008758768, 0.009295491, 0.009634088, 0.028728955, 0.03341760, 0.0195634463],\n",
        "    [0.006219993, 0.004758676, 0.02356259, 0.00284857, 0.023670257, 0.02508310, 0.14204710, 0.128333, 0.02582307],\n",
        "    [0.01148842, 0.00997528, 0.089486699, 0.0152268, 0.019967049, 0.0198038678, 0.165155677, 0.137869267, 0.029367855]\n",
        "])\n",
        "eps_v2_exact = eps_v2_exact[a:b, c:d]\n",
        "\n",
        "\n",
        "eps_v2_app = np.array([\n",
        "    [0.0032412139, 0.0032412139, 0.0032412139, 0.0032412139, 0.0032412139, 0.0032412139, 0.0027744548, 0.002780725, 0.00300493],\n",
        "    [0.0032552085, 0.00217699, 0.003022389, 0.002458995, 0.003304652, 0.003386966, 0.047186947, 0.049938408, 0.01818605],\n",
        "    [0.01382715, 0.011163517, 0.02250201, 0.008934338, 0.00949379, 0.009828665, 0.0289486374, 0.03363103, 0.0197738062],\n",
        "    [0.009778753, 0.00804437, 0.019877499, 0.006027446, 0.032246767, 0.035204712, 0.177783456, 0.16698050, 0.03484607],\n",
        "    [0.01311337, 0.01565665, 0.07786943, 0.01048145, 0.033328745, 0.0344475977, 0.23392218, 0.20878358, 0.0474685]\n",
        "])\n",
        "eps_v2_app = eps_v2_app[a:b, c:d]\n",
        "\n",
        "eps = np.array([\n",
        "    [0.0033079, 0.003308, 0.00330791, 0.0033079, 0.0033079, 0.0033079, 0.0028361, 0.00284251, 0.0030695],\n",
        "    [0.0032497, 0.002168, 0.00301, 0.002449, 0.00330, 0.00339, 0.047050, 0.049808, 0.018174],\n",
        "    [0.0137506, 0.011032, 0.022746, 0.008771, 0.009307, 0.009646, 0.02869, 0.03334, 0.01956],\n",
        "    [0.006223, 0.004761, 0.023572, 0.002850, 0.023673, 0.025086, 0.142037, 0.128308, 0.025828],\n",
        "    [0.011495, 0.009978, 0.08958, 0.015229, 0.019971, 0.019808, 0.165136, 0.1378337, 0.0293757]\n",
        "])\n",
        "eps = eps[a:b, c:d]\n",
        "\n",
        "\n",
        "# Creating subplots\n",
        "fig, axes = plt.subplots(1, 2, figsize=(11, 4))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "y = eps.flatten()\n",
        "y = y/max(y)\n",
        "x1 = eps_v2_exact.flatten()\n",
        "x1 = x1/max(x1)\n",
        "x2 = eps_v2_app.flatten()\n",
        "x2 = x2/max(x2)\n",
        "\n",
        "\n",
        "#Obtaining correlation coefficients\n",
        "r1, _ = pearsonr(list(x1), list(y))\n",
        "r2, _ = pearsonr(list(x2), list(y))\n",
        "\n",
        "\n",
        "\n",
        "axes[1].scatter(x1, y, alpha=0.7, marker='.', c='r', label = '$\\epsilon_{\\\\text{2}}$ correlation = %.2f'%r1)\n",
        "axes[1].scatter(x2, y, alpha=0.7, marker='*', c='b', label = '$\\epsilon_{\\\\text{app}}$ correlation = %.2f'%r2)\n",
        "\n",
        "axes[1].set_xlim(0,1.1)\n",
        "axes[1].set_ylim(0,1.1)\n",
        "\n",
        "# Fit and plot a trendline (optional)\n",
        "m1, b1 = np.polyfit(x1, y, 1)\n",
        "axes[1].plot(x1, m1 * x1 + b1, color='red', alpha = 0.3, linestyle='-')\n",
        "\n",
        "m2, b2 = np.polyfit(x2, y, 1)\n",
        "axes[1].plot(x2, m2 * x2 + b2, color='blue', linestyle='-')\n",
        "\n",
        "axes[1].set_xlabel(\"Perturbation based Trotter error\", fontsize=14)\n",
        "# axes[1].set_ylabel(\"Exact Trotter error ($\\epsilon$)\", fontsize=14)\n",
        "axes[1].legend()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Time steps\n",
        "dt = np.array([\n",
        "    [0.908141],\n",
        "    [0.128461],\n",
        "    [0.064592],\n",
        "    [0.01337],\n",
        "    [0.018123]\n",
        "])\n",
        "\n",
        "\n",
        "# alpha\n",
        "alpha = np.array([\n",
        "    [0.019917, 0.016297, 0.02223, 0.016297, 0.016296, 0.016297, 0.015327, 0.015319, 0.015776],\n",
        "    [1.071179, 0.261750, 0.62713, 0.260248, 0.130109, 0.117276, 0.523921, 0.461217, 0.230157],\n",
        "    [4.22008, 1.023566, 4.96021, 0.986088, 0.580984, 0.550564, 2.359864, 2.03442, 1.12533],\n",
        "    [79.40951, 28.73135, 181.5595, 27.85735, 15.30047, 15.05653, 52.37091, 48.27083, 27.88276],\n",
        "    [51.66221, 16.36468, 65.99235, 16.0206, 7.81380, 7.640403, 28.428811, 25.78507, 14.50743]\n",
        "])\n",
        "alpha = alpha[a:b, c:d]\n",
        "alpha_dt2 = alpha\n",
        "\n",
        "\n",
        "alpha_e = np.array([\n",
        "    [0.018603, 0.011459, 0.01860, 0.011459, 0.011459, 0.011459, 0.01111, 0.01111, 0.011299],\n",
        "    [0.22472, 0.180268, 0.2506, 0.180367, 0.10034, 0.09980, 0.07223, 0.08325, 0.06039],\n",
        "    [0.672117, 0.75365, 0.80521, 0.756595, 0.42353, 0.42957, 0.29231, 0.34624, 0.35711],\n",
        "    [23.25445, 23.42057, 46.44921, 23.44921, 1.84682, 11.86137, 15.66702, 14.87779, 12.86540],\n",
        "    [11.22894, 11.22535, 15.86476, 11.21336, 6.552417, 6.559554, 7.254640, 7.048216, 6.025332]\n",
        "])\n",
        "alpha_e = alpha_e[a:b, c:d]\n",
        "alpha_e_dt2 = alpha_e\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "yy = eps.flatten()\n",
        "yy = yy/max(yy)\n",
        "xx1 = alpha_dt2.flatten()\n",
        "xx1 = xx1/max(xx1)\n",
        "xx2 = alpha_e_dt2.flatten()\n",
        "xx2 = xx2/max(xx2)\n",
        "\n",
        "\n",
        "rr1, _ = pearsonr(list(xx1), list(yy))\n",
        "rr2, _ = pearsonr(list(xx2), list(yy))\n",
        "\n",
        "\n",
        "axes[0].scatter(xx1, yy, alpha=0.7, marker='.', c='r', label = '$\\\\alpha$ correlation = %.2f'%rr1)\n",
        "axes[0].scatter(xx2, yy, alpha=0.7, marker='*', c='b', label = '$\\\\alpha_e$ correlation = %.2f'%rr2)\n",
        "\n",
        "axes[0].set_xlim(0,1.1)\n",
        "axes[0].set_ylim(0,1.1)\n",
        "\n",
        "# Fit and plot a trendline (optional)\n",
        "m1, b1 = np.polyfit(xx1, yy, 1)\n",
        "axes[0].plot(xx1, m1 * xx1 + b1, color='red', alpha = 0.7, linestyle='-')\n",
        "\n",
        "m2, b2 = np.polyfit(xx2, yy, 1)\n",
        "axes[0].plot(xx2, m2 * xx2 + b2, color='blue', linestyle='-')\n",
        "\n",
        "axes[0].set_xlabel(\"Operator norm based Trotter error\", fontsize=14)\n",
        "axes[0].set_ylabel(\"Exact Trotter error ($\\epsilon$)\", fontsize=14)\n",
        "axes[0].legend()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "axes[1].text(-0.01, 1.02, \"a)\", transform=axes[1].transAxes, fontsize=14, fontweight=\"bold\", va=\"top\", ha=\"right\")\n",
        "axes[0].text(-0.01, 1.02, \"b)\", transform=axes[0].transAxes, fontsize=14, fontweight=\"bold\", va=\"top\", ha=\"right\")\n",
        "\n",
        "\n",
        "\n",
        "fig.tight_layout()\n",
        "\n",
        "# #To save the figure, uncomment the following if working on google colab\n",
        "# from google.colab import files\n",
        "# plt.savefig('Trotter_error_correlation_plot.pdf')\n",
        "# files.download('Trotter_error_correlation_plot.pdf')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "dXIhbnZNbeVt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mdQWV4WPAICN"
      },
      "source": [
        "## T gate counts\n",
        "Calculate upperbound on T gates needed to reach chemical accuracy in obtaining ground state energy of molecular Hamiltonians under QPE + Trotter approximation algorithm for various Hamiltonian fragmentation techniques."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Finding the best T gates by exploring various initial values for the optimizer\n",
        "\n",
        "Since the function that optimizes T gate count is highly sensitive to inital parameters, we optimize T gates for various initial parameters on a grid to get a better estimate. As it is a nonlinear optimization, each run could yield a different result, but the relative error will be negligible.\\\n",
        "_Caution: This could take a while to run_"
      ],
      "metadata": {
        "id": "AzvUhxl8_dIN"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hGrjCgnUAJ1l"
      },
      "outputs": [],
      "source": [
        "eps_v2_exact = np.array([\n",
        "    [0.0032412139, 0.0032412139, 0.0032412139, 0.0032412139, 0.0032412139, 0.0032412139, 0.0027744548, 0.002780725, 0.00300493],\n",
        "    [0.0032480968, 0.002163644, 0.0030127983, 0.00244501, 0.003300397, 0.003386966, 0.04718898, 0.049932963, 0.01818811],\n",
        "    [0.013731045, 0.011020856, 0.02272475, 0.008758768, 0.009295491, 0.009634088, 0.028728955, 0.03341760, 0.0195634463],\n",
        "    [0.006219993, 0.004758676, 0.02356259, 0.00284857, 0.023670257, 0.02508310, 0.14204710, 0.128333, 0.02582307],\n",
        "    [0.01148842, 0.00997528, 0.089486699, 0.0152268, 0.019967049, 0.0198038678, 0.165155677, 0.137869267, 0.029367855]\n",
        "])\n",
        "\n",
        "\n",
        "eps_v2_app = np.array([\n",
        "    [0.0032412139, 0.0032412139, 0.0032412139, 0.0032412139, 0.0032412139, 0.0032412139, 0.0027744548, 0.002780725, 0.00300493],\n",
        "    [0.0032552085, 0.00217699, 0.003022389, 0.002458995, 0.003304652, 0.003386966, 0.047186947, 0.049938408, 0.01818605],\n",
        "    [0.01382715, 0.011163517, 0.02250201, 0.008934338, 0.00949379, 0.009828665, 0.0289486374, 0.03363103, 0.0197738062],\n",
        "    [0.009778753, 0.00804437, 0.019877499, 0.006027446, 0.032246767, 0.035204712, 0.177783456, 0.16698050, 0.03484607],\n",
        "    [0.01311337, 0.01565665, 0.07786943, 0.01048145, 0.033328745, 0.0344475977, 0.23392218, 0.20878358, 0.0474685]\n",
        "])\n",
        "\n",
        "\n",
        "alpha = np.array([\n",
        "    [0.019917, 0.016297, 0.02223, 0.016297, 0.016296, 0.016297, 0.015327, 0.015319, 0.015776],\n",
        "    [1.071179, 0.261750, 0.62713, 0.260248, 0.130109, 0.117276, 0.523921, 0.461217, 0.230157],\n",
        "    [4.22008, 1.023566, 4.96021, 0.986088, 0.580984, 0.550564, 2.359864, 2.03442, 1.12533],\n",
        "    [79.40951, 28.73135, 181.5595, 27.85735, 15.30047, 15.05653, 52.37091, 48.27083, 27.88276],\n",
        "    [51.66221, 16.36468, 65.99235, 16.0206, 7.81380, 7.640403, 28.428811, 25.78507, 14.50743]\n",
        "])\n",
        "\n",
        "\n",
        "#Number of fermionic fragments of each type for each molecule.\n",
        "ffrag_lens = np.array([\n",
        "    [  5,   4,   5,   4,   9],\n",
        "    [ 23,  51,  23,  51, 113],\n",
        "    [ 30,  71,  30,  71, 157],\n",
        "    [ 28,  62,  28,  62, 117],\n",
        "    [ 35,  99,  35,  99, 169]\n",
        "])\n",
        "\n",
        "\n",
        "\n",
        "mol_names = ['h2', 'lih', 'beh2', 'h2o', 'nh3']\n",
        "all_methods = ['qwclf', 'qwcsi', 'fclf', 'fcsi', 'lrlcu', 'gfrolcu', 'lr', 'gfro', 'sdgfro']\n",
        "rs = [1, 1, 1, 1.9, 1.9]\n",
        "scale = 1e8\n",
        "Tot_Err = 1.6e-3\n",
        "already_tapered = False\n",
        "n_samps = 40\n",
        "\n",
        "#Arrays to store T gate estimates\n",
        "eps_v2_exact_T_count = np.zeros((len(mol_names), len(all_methods)))\n",
        "eps_v2_app_T_count = np.zeros((len(mol_names), len(all_methods)))\n",
        "alpha_T_count = np.zeros((len(mol_names), len(all_methods)))\n",
        "\n",
        "#Finding T gates for all the samples in one run\n",
        "for i, mol_name in enumerate(mol_names):\n",
        "  r = rs[i]\n",
        "  H, num_elecs = obtain_OF_hamiltonian(mol_name, geometry=r)\n",
        "\n",
        "  H_const, H_obt, H_tbt = get_chem_tensors(H)\n",
        "  H_obt_op, H_tbt_op = obt2op(H_obt), tbt2op(H_tbt)\n",
        "  Hchem = H_obt_op + H_tbt_op + H_const\n",
        "\n",
        "  for j, method in enumerate(all_methods):\n",
        "    if j < 4:\n",
        "      if (mol_name != 'nh3'):\n",
        "        JW_OF = jordan_wigner(Hchem)\n",
        "        n_qubits = count_qubits(JW_OF)\n",
        "        already_tapered = False\n",
        "\n",
        "      #Perform tapering if it is nh3 and method is a qubit method\n",
        "      if (mol_name == 'nh3') and (not already_tapered):\n",
        "        JW_OF = taper_qubits(JW_OF, n_qubits, int(num_elecs/2), int(num_elecs/2))\n",
        "        n_qubits = count_qubits(JW_OF)\n",
        "        already_tapered = True\n",
        "\n",
        "      JW_keys = JW_OF.terms.keys()\n",
        "      Nr = 2*len(JW_keys)                                                     # *2 because we are working with 2nd order Trotter\n",
        "    else:\n",
        "      n_qubits = count_qubits(H)\n",
        "      Nr = countOneRotFerm(n_qubits, ffrag_lens[i, j-4], ord=2)\n",
        "\n",
        "    print (f\"Currently doing {mol_name} with {method}\")\n",
        "\n",
        "    err_est = eps_v2_exact[i, j]\n",
        "    global_optimal_errs, global_min_Nt = explore_params(n_samps, Tot_Err, err_est, Nr, scale, use_constraints = False)\n",
        "    if sum(global_optimal_errs) - Tot_Err > 1e-8:\n",
        "      raise ValueError(\"Sum of global_optimal_errs exceeds Tot_Err\")\n",
        "    eps_v2_exact_T_count[i, j] = global_min_Nt\n",
        "    print (global_min_Nt)\n",
        "\n",
        "    err_est = eps_v2_app[i, j]\n",
        "    global_optimal_errs, global_min_Nt = explore_params(n_samps, Tot_Err, err_est, Nr, scale, use_constraints = False)\n",
        "    if sum(global_optimal_errs) - Tot_Err > 1e-8:\n",
        "      raise ValueError(\"Sum of global_optimal_errs exceeds Tot_Err\")\n",
        "    eps_v2_app_T_count[i, j] = global_min_Nt\n",
        "    print (global_min_Nt)\n",
        "\n",
        "    err_est = alpha[i, j]\n",
        "    global_optimal_errs, global_min_Nt = explore_params(n_samps, Tot_Err, err_est, Nr, scale, use_constraints = False)\n",
        "    if sum(global_optimal_errs) - Tot_Err > 1e-8:\n",
        "      raise ValueError(\"Sum of global_optimal_errs exceeds Tot_Err\")\n",
        "    alpha_T_count[i, j] = global_min_Nt\n",
        "    print (global_min_Nt)\n",
        "\n",
        "    print (f\"Completed T gate count for {mol_name} with {method} \\n\")\n",
        "\n",
        "\n",
        "np.set_printoptions(linewidth=95)\n",
        "np.set_printoptions(formatter={'float': '{:.2e}'.format})\n",
        "print (np.matrix(alpha_T_count))\n",
        "print (np.matrix(eps_v2_exact_T_count))\n",
        "print (np.matrix(eps_v2_app_T_count))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Printing and comparing the best methods based on T gate estimates"
      ],
      "metadata": {
        "id": "ySpyjiN1_kbO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "np.set_printoptions(linewidth=95)\n",
        "np.set_printoptions(formatter={'float': '{:.2e}'.format})\n",
        "# print (np.matrix(alpha_T_count))\n",
        "# print (np.matrix(eps_v2_exact_T_count))\n",
        "# print (np.matrix(eps_v2_app_T_count))\n",
        "\n",
        "\n",
        "methods = ['qwc lf', 'qwc si', 'fc lf', 'fc si', 'lr lcu', 'gfro lcu', 'lr', 'gfro', 'sd gfro']\n",
        "mol_names = ['h2', 'lih', 'beh2', 'h2o', 'nh3']\n",
        "\n",
        "alpha_T_count = np.array([\n",
        "    [1.56e+07, 1.40e+07, 1.65e+07, 1.40e+07, 1.39e+08, 1.05e+08, 1.35e+08, 1.01e+08, 2.76e+08],\n",
        "    [6.22e+09, 2.99e+09, 4.71e+09, 2.98e+09, 2.31e+10, 5.10e+10, 4.75e+10, 1.04e+11, 1.66e+11],\n",
        "    [1.34e+10, 6.42e+09, 1.46e+10, 6.30e+09, 9.17e+10, 2.21e+11, 1.89e+11, 4.35e+11, 7.31e+11],\n",
        "    [1.02e+11, 6.02e+10, 1.56e+11, 5.93e+10, 4.63e+11, 1.06e+12, 8.75e+11, 1.94e+12, 2.83e+12],\n",
        "    [8.16e+10, 4.50e+10, 9.27e+10, 4.45e+10, 5.46e+11, 1.60e+12, 1.06e+12, 3.01e+12, 3.89e+12]\n",
        "])\n",
        "\n",
        "\n",
        "\n",
        "eps_v2_exact_T_count = np.array([\n",
        "    [6.22e+06, 6.22e+06, 6.22e+06, 6.22e+06, 6.19e+07, 4.65e+07, 5.70e+07, 4.29e+07, 1.20e+08],\n",
        "    [3.13e+08, 2.53e+08, 3.01e+08, 2.70e+08, 3.52e+09, 8.32e+09, 1.40e+10, 3.36e+10, 4.58e+10],\n",
        "    [7.01e+08, 6.25e+08, 9.11e+08, 5.55e+08, 1.10e+10, 2.79e+10, 1.98e+10, 5.31e+10, 9.21e+10],\n",
        "    [7.72e+08, 6.72e+08, 1.55e+09, 5.14e+08, 1.66e+10, 3.97e+10, 4.21e+10, 9.25e+10, 7.83e+10],\n",
        "    [1.06e+09, 9.88e+08, 3.09e+09, 1.23e+09, 2.55e+10, 7.56e+10, 7.61e+10, 2.06e+11, 1.62e+11]\n",
        "])\n",
        "\n",
        "\n",
        "\n",
        "eps_v2_app_T_count = np.array([\n",
        "    [6.22e+06, 6.22e+06, 6.22e+06, 6.22e+06, 6.19e+07, 4.65e+07, 5.70e+07, 4.29e+07, 1.20e+08],\n",
        "    [3.13e+08, 2.54e+08, 3.01e+08, 2.70e+08, 3.52e+09, 8.32e+09, 1.40e+10, 3.36e+10, 4.58e+10],\n",
        "    [7.04e+08, 6.29e+08, 9.07e+08, 5.60e+08, 1.11e+10, 2.82e+10, 1.99e+10, 5.33e+10, 9.26e+10],\n",
        "    [9.78e+08, 8.83e+08, 1.41e+09, 7.60e+08, 1.95e+10, 4.73e+10, 4.73e+10, 1.06e+11, 9.14e+10],\n",
        "    [1.14e+09, 1.25e+09, 2.88e+09, 1.01e+09, 3.32e+10, 1.01e+11, 9.11e+10, 2.56e+11, 2.07e+11]\n",
        "])\n",
        "\n",
        "best_alpha_T_count = np.zeros((5, 3))\n",
        "best_eps_v2_exact_T_count = np.zeros((5, 3))\n",
        "best_eps_v2_app_T_count = np.zeros((5, 3))\n",
        "\n",
        "n=3                                                                     #Choose best 3 fragmentation\n",
        "for i in range (5):\n",
        "  print (f\"Molecule = {mol_names[i]}\")\n",
        "\n",
        "  eps_v2_exact_T_count_i = eps_v2_exact_T_count[i, :]\n",
        "  # Step 1: Get indices\n",
        "  smallest_indices = np.argpartition(eps_v2_exact_T_count_i, n)[:n]\n",
        "  # Step 2: Sort those indices by actual value\n",
        "  smallest_indices = smallest_indices[np.argsort(eps_v2_exact_T_count_i[smallest_indices])]\n",
        "  # Step 3: Get the corresponding smallest values\n",
        "  smallest_values = eps_v2_exact_T_count_i[smallest_indices]\n",
        "  best_eps_v2_exact_T_count[i, :] = smallest_values\n",
        "  print ([methods[smallest_indices[j]].upper() + \" $({:.2e}\".format(smallest_values[j])[:6]+'\\times 10^{'+\"{:.2e}\".format(smallest_values[j])[-2:]+\"})$\" for j in range(3)])\n",
        "\n",
        "  alpha_T_count_i = alpha_T_count[i, :]\n",
        "  # Step 1: Get indices of n smallest values (unsorted)\n",
        "  smallest_indices = np.argpartition(alpha_T_count_i, n)[:n]\n",
        "  # Step 2: Sort those indices by actual value\n",
        "  smallest_indices = smallest_indices[np.argsort(alpha_T_count_i[smallest_indices])]\n",
        "  # Step 3: Get the corresponding smallest values\n",
        "  smallest_values = alpha_T_count_i[smallest_indices]\n",
        "  best_alpha_T_count[i, :] = smallest_values\n",
        "  print ([methods[smallest_indices[j]].upper() + \" $({:.2e}\".format(smallest_values[j])[:6]+'\\times 10^{'+\"{:.2e}\".format(smallest_values[j])[-2:]+\"})$\" for j in range(3)])\n",
        "\n",
        "\n",
        "  eps_v2_app_T_count_i = eps_v2_app_T_count[i, :]\n",
        "  # Step 1: Get indices\n",
        "  smallest_indices = np.argpartition(eps_v2_app_T_count_i, n)[:n]\n",
        "  # Step 2: Sort those indices by actual value\n",
        "  smallest_indices = smallest_indices[np.argsort(eps_v2_app_T_count_i[smallest_indices])]\n",
        "  # Step 3: Get the corresponding smallest values\n",
        "  smallest_values = eps_v2_app_T_count_i[smallest_indices]\n",
        "  best_eps_v2_app_T_count[i, :] = smallest_values\n",
        "  print ([methods[smallest_indices[j]].upper() + \" $({:.2e}\".format(smallest_values[j])[:6]+'\\times 10^{'+\"{:.2e}\".format(smallest_values[j])[-2:]+\"})$\" for j in range(3)])\n",
        "  print ('\\n\\n')\n",
        "\n",
        "# print (best_alpha_T_count)\n",
        "# print (best_eps_v2_exact_T_count)\n",
        "# print (best_eps_v2_app_T_count)"
      ],
      "metadata": {
        "id": "KKBUO8sJob0w"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "DwaNvGpPnWII",
        "mdQWV4WPAICN",
        "AzvUhxl8_dIN",
        "ySpyjiN1_kbO"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}